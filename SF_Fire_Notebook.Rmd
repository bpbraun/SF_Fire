---
title: "SF Fire Analysis"
Author: "Benjamin Braun, benjaminpbraun@gmail.com"
output: html_notebook
---

The following is my notebook for approaching these problem.  It is set up in a way that it can be easily shared and produce reproducible results. Under normal circumstances, I would make a client ready version that I would export as an HTML or PDF without code.  If time permits, I'll include that as well.

## Questions/Tasks

Task 1) There are perceived trends that tend to occur effecting turnout performance.  Your task is to explore the validity of each trend.  For each trend, present analysis in the form of visualization (chart, table, etc).

    * Evening incidents have slower turnout times.  Note: Evening incidents are incidents that are created between the hours of 10:00 PM and 6:00 AM.
    * Units with back to back responses have slower turnout times.  A back to back response is defined by a unit being dispatched within 10 minutes of becoming available.

Task 2) are there additional trends that would be helpful for the department to know? You may use additional datasources if interest

## Loading the data

The first issue is getting the data into a manageable size for analysis.  The whole data set is far too large to handle on my laptop, so I'll work with the 2016 calls only.  This file is itself 315.3 MB which is far above the recommended size for github and also too large for me to handle effectively on my laptop.  Therefore, I'm going to take a sample of rows from this file.  This will provide some intuition regarding the questions that we can then explore in an environment better suited for handling such a large data set.  Furthermore, conducting analysis on a sample before testing on the the larger body of data is a better way to ensure unbiased results and make sure we don't find patterns in the data that don't actually exist.

With that in mind, I'm going to read in the first 10,000 rows of data and perform the preliminary analysis on this subset.  I'll also write this out as a CSV to post on github and use for the remainder of the analysis.

```{r random sample}

library(tidyverse)

# This is the original code I used to read in a sample of the data set.  I'm keeping it here for reproducibility.
# 
# sample_data <- read_csv("Fire_Department_Calls_For_Service__2016_.csv",
#                         n_max = 10000)
# 
# write_csv(sample_data, "fire_subset.csv")

sample_data <- read_csv("fire_subset.csv") # this writes the actual file


```

Before moving on, I want to mention some issues inherent with taking the first 10,000 rows as my sample.  This will only reflect data from midnight on January 1st through 2 PM on January 12th.  I don't know if this is a particularly busy or slow time (I would think that new years there may be a particularly busy time; conversely there may be fewer fires in the winter).  Under normal circumstances, I would either select rows at random or better yet, work with the client to determine a time-frame that they feel is most appropriate.

## Evening Incidents

### Exploring and Cleaning

Let's explore the data.

```{r data glimpse}

glimpse(sample_data)

```

For the first task, we're interested in turnout time (__the duration of time between when a unit is dispatched and when the unit is enroute__) at night.  So let's select the appropriate columns and then filter for night.  I'm going to assume that `Dispatch DtTm` is the time that the unit is dispatched and that `Response DtTm` is the time they are enroute (under normal circumstances I would confirm this with the client.)

In the following code I will:

* Select the appropriate variables
* Filter out NA responses
* Convert the date/time columns from character strings to date_time
* Add a column indicating whether the call was durring night (10 AM to 6 PM) or not

```{r select and clean data}

library(lubridate)

data_night <- sample_data %>% 
  select(RowID, `Call Number`, `Dispatch DtTm`, `Response DtTm`) %>%  # select colums
  filter(!is.na(`Response DtTm`) == T) %>%  # filter out NA
  mutate(`Dispatch DtTm` = mdy_hms(`Dispatch DtTm`),
         `Response DtTm` = mdy_hms(`Response DtTm`)) %>% # change columns to mdy_hms format from character
  mutate(difference = `Response DtTm` - `Dispatch DtTm`) %>% # make a column with the difference
  mutate(just_time = hms(format(ymd_hms(`Dispatch DtTm`), "%H:%M:%S"))) %>% # make column with just the time
  mutate(night_day = if_else(just_time >= hms("20:00:00") | just_time <= hms("06:00:00"), # mark as day or night
                     "night",
                     "day"))
                     
data_night

```

Before Visualizing, let's see on average if night response time are greater.

```{r night response times}

data_night %>% 
  group_by(night_day) %>% 
  summarise(average = mean(difference), median = median(difference))

```

So on average night does have a higher response time (both mean and median), but let's see if that holds up when we analyze it visually.

### Visualize

Now let's visualize the results.

```{r visualize data, warning=FALSE, message=FALSE}

ggplot(data_night, aes(x = as.numeric(just_time), 
                       y = difference, 
                       color = night_day)) +
  geom_point() +
  labs(y = "Response Time in Seconds",
       x = "Time of Day",
       title = "Except for outliers, night response times do not significantly differ from day response times") +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.x = element_blank())

```

### Interpret

While the topline numbers for median and mean response times are, in fact, higher for night responses, visualizing the results show that they are actually pretty simmilar; only a few outliers are changing those numbers.

In a full analysis, the next steps would be to perform some statistical testing to see if this difference is actually statistically significant.

## Back to Back Responses

For the second task, we're interested in back to back response times (__back to back response is defined by a unit being dispatched within 10 minutes of becoming available__). So let's select the appropriate columns and then filter for night.  We'll again use `Dispatch DtTm` as the time that the unit is dispatched and `Response DtTm` as the time they are enroute (under normal circumstances I would confirm this with the client.)  We'll use `Available DtTm` for the time they became available.

In the following code I will:

* Select the appropriate variables
* Filter out NA responses
* Convert the date/time columns from character strings to date_time
* Add a column indicating whether the call was within 10 minutes of becoming available.

### Exploring and Cleaning

```{r select and clean data 2}

data_available <- sample_data %>% 
  select(RowID, `Call Number`, `Dispatch DtTm`, `Response DtTm`, `Available DtTm`) %>%  # select colums
  filter(!is.na(`Response DtTm`) == T,
         !is.na(`Available DtTm`) == T) %>%  # filter out NA
  mutate(`Dispatch DtTm` = mdy_hms(`Dispatch DtTm`),
         `Response DtTm` = mdy_hms(`Response DtTm`),
         `Available DtTm` = mdy_hms(`Available DtTm`)) %>% # change columns to mdy_hms format from character
  mutate(difference = `Response DtTm` - `Dispatch DtTm`) %>% # make a column with the difference
  mutate(diff_avail = `Available DtTm` - `Dispatch DtTm`) %>% 
  mutate(back_to_back = if_else(diff_avail <= time_length(600),
                     "Yes",
                     "No"))
                     
data_available
```

Before Visualizing, let's see on average if night response time are greater.

```{r back to back response times}

data_available %>% 
  group_by(back_to_back) %>% 
  summarise(average = mean(difference), median = median(difference))

```

As with the night response time, back to back response times also appear to have higher mean and median responses.  Let's see if visualizing the results give us additional insight.

Visualize

```{r visualize data 2, warning=FALSE, message=FALSE}

ggplot(data_available, aes(x = as.numeric(diff_avail), 
                       y = difference, 
                       color = back_to_back)) +
  geom_point() + 
  labs(y = "Response Time in Seconds",
       x = "Difference Between Availability and Dispatch",
       title = "Availability appears to affect responsen time beyond 10 minutes") +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())

```

### Intepret

Similar to the night topline numbers for median and mean response times, visualizing the results for back to back respoinses  show that they are actually pretty simmilar to non back to back respones.  It seems that response time is affected by rest for far longoer than 10 minutes; moreover, variation seems extremely wide when there is more rest.

In a full analysis, the next steps would be to perform some statistical testing to see if this difference is actually statistically significant.

## Additional Analysis

One additional analysis would be to analyze the results by neighborhood. This could be useful to see if certain areas are being served better than others.

```{r responses by neighborhood}

data_neighborhood <- sample_data %>% 
  select(RowID, `Dispatch DtTm`, `Response DtTm`, `Neighborhooods - Analysis Boundaries`) %>%  # select colums
  filter(!is.na(`Response DtTm`) == T,
         !is.na(`Neighborhooods - Analysis Boundaries`)) %>%  # filter out NA
  mutate(`Dispatch DtTm` = mdy_hms(`Dispatch DtTm`),
         `Response DtTm` = mdy_hms(`Response DtTm`)) %>% # change columns to mdy_hms format from character
  mutate(difference = `Response DtTm` - `Dispatch DtTm`) %>% # make a column with the difference
  mutate(just_time = hms(format(ymd_hms(`Dispatch DtTm`), "%H:%M:%S"))) %>% # make column with just the time
  group_by(`Neighborhooods - Analysis Boundaries`) %>%
  summarise(average = mean(difference)) %>% 
  ungroup() %>% 
  arrange(desc(average)) %>% 
  rename(Neighborhood = `Neighborhooods - Analysis Boundaries`)
                     
data_neighborhood

ggplot(data_neighborhood, aes(x = reorder(Neighborhood, average), y = average)) +
  geom_col()+
  coord_flip() + 
  labs(x = "Neighborhood",
       y = "Average Response Time",
       title = "Response times vary greatly by neighborhood") +
  theme_minimal() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())

```

A subsequent phase of analysis would be to averlay this information on a map with other geospatial data like the location of fire stations and deomgraphic data (socio-economic, ethnic, etc.).
